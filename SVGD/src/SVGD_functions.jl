using DrWatson
using ProgressMeter
using Statistics
using ValueHistories
using KernelFunctions
using LinearAlgebra
using Random
using Flux
using ForwardDiff
using Distances
using PDMats

export svgd_fit
export calculate_phi_vectorized
export compute_dKL
export kernel_grad_matrix
export calculate_phi

"""
Fit the samples in q to the distribution corresponding to grad_logp.
Possible values for dKL_estimator are `:RKHS_norm`, `:KSD`, `:UKSD`; they can be
combined by putting them in array.
Possible values for update_method are `:forward_euler`, `:naive_WNES`,
':scalar_Adam', ':scalar_RMS_prop', ':scalar_adagrad' `:naive_WAG`.
"""
function svgd_fit(q, grad_logp; kernel, kwargs...)
    kwargs = Dict(kwargs...)
    callback = get(kwargs, :callback, nothing)
    n_iter = get(kwargs, :n_iter, 1)
    n_particles = get(kwargs, :n_particles, 1)
    step_size = get(kwargs, :step_size, 1)
    kernel_cb! = get!(kwargs, :kernel_cb, nothing)
    step_size_cb = get!(kwargs, :step_size_cb, nothing)
    update_method = get!(kwargs, :update_method, :forward_euler)
    annealing_schedule = get!(kwargs, :annealing_schedule, nothing)
    annealing_params = get!(kwargs, :annealing_params, [])
    progress = get(kwargs, :progress, true)

    aux_vars = Dict()
    if update_method in [:scalar_adagrad, :scalar_RMS_prop]
        aux_vars[:G‚Çú] = [0.]
    elseif update_method == :scalar_Adam
        aux_vars[:m‚Çú] = zeros(size(q))
        aux_vars[:m‚Çú‚Çã‚ÇÅ] = zeros(size(q))
        aux_vars[:v‚Çú] = zeros(size(q))
        aux_vars[:ùîº‚àám‚Çú‚Çã‚ÇÅ] = [0.]
        aux_vars[:ùîº‚àáœï‚Çú‚Çã‚ÇÅ] = [0.]
    elseif update_method in [:naive_WAG, :naive_WNES]
        aux_vars[:y] = copy(q)
        aux_vars[:q‚Çú‚Çã‚ÇÅ] = copy(q)
        aux_vars[:q‚Çú‚Çã‚ÇÇ] = copy(q)
    end
    hist = MVHistory()
    œï = zeros(size(q))
    progress ? p = Progress(n_iter, 1) : nothing
    for i in 1:n_iter
        isnothing(kernel_cb!) ? nothing : kernel_cb!(kernel, q)
        œµ = isnothing(step_size_cb) ? [step_size] : [step_size_cb(step_size, i)]
        Œ≥‚Çê = if isnothing(annealing_schedule)
            [1.]
        else
            [annealing_schedule(i, n_iter; annealing_params...)]
        end
        update!(Val(update_method), q, œï, œµ, kernel, grad_logp, aux_vars,
                iter=i, Œ≥‚Çê=Œ≥‚Çê; kwargs...)
        push_to_hist!(hist, q, œµ, œï, i, Œ≥‚Çê, kernel, grad_logp, aux_vars; kwargs...)
        if !isnothing(callback)
            callback(;hist=hist, q=q, œï=œï, i=i, kernel=kernel,
                     grad_logp=grad_logp, aux_vars..., kwargs...)
        end
        progress ? next!(p) : nothing
    end
    return q, hist
end

function push_to_hist!(
    hist, q, œµ, œï, i, Œ≥‚Çê, kernel, grad_logp, aux_vars,
    ; kwargs...
)
    push!(hist, :step_sizes, i, œµ[1])
    push!(hist, :annealing, i, Œ≥‚Çê[1])
    push!(hist, :œï_norm, i, mean(norm(œï)))

    dKL_estimator = get(kwargs, :dKL_estimator, false)
    if kwargs[:update_method] == :naive_WNES
        dKL = WNes_dKL(kernel, q, œï, grad_logp, aux_vars, œµ; kwargs...)
        push!(hist, :dKL, i, dKL)
    elseif kwargs[:update_method] == :scalar_Adam
        dKL = dKL_Adam(kernel, q, œï, grad_logp, aux_vars, œµ; kwargs...)
        push!(hist, :adam_dKL, i, dKL)
    elseif typeof(dKL_estimator) == Symbol
        dKL = compute_dKL(Val(dKL_estimator), kernel, q, œï=œï,
                          grad_logp=grad_logp)
        dKL += dKL_annealing_correction(œï, grad_logp, q, Œ≥‚Çê)
        push!(hist, dKL_estimator, i, dKL)
    elseif typeof(dKL_estimator) == Array{Symbol,1}
        for estimator in dKL_estimator
            dKL = compute_dKL(Val(estimator), kernel, q, œï=œï, grad_logp=grad_logp)
            dKL += dKL_annealing_correction(œï, grad_logp, q, Œ≥‚Çê)
            push!(hist, estimator, i, dKL)
        end
    end
    push!(hist, :kernel_width, kernel.transform.s)
end

function dKL_annealing_correction(œï, grad_logp, q, Œ≥‚Çê)
    c = 0
    for (x·µ¢, œï·µ¢) in zip(eachcol(œï), eachcol(q))
        c += dot(œï·µ¢, grad_logp(x·µ¢))
    end
    -(1-Œ≥‚Çê[1])*c / size(q)[2]
end

function dKL_Adam(kernel, q, œï, grad_logp, aux_vars, œµ; kwargs...)
    Œ≤‚ÇÅ = get(kwargs, :Œ≤‚ÇÅ, false)
    Œ≤‚ÇÇ = get(kwargs, :Œ≤‚ÇÇ, false)
    d, N = size(q)
    m‚Çú‚Çã‚ÇÅ = aux_vars[:m‚Çú‚Çã‚ÇÅ]
    ùîº‚àám‚Çú‚Çã‚ÇÅ = aux_vars[:ùîº‚àám‚Çú‚Çã‚ÇÅ]
    ùîº‚àáœï‚Çú‚Çã‚ÇÅ = aux_vars[:ùîº‚àáœï‚Çú‚Çã‚ÇÅ]
    ùîº‚àám‚Çú‚Çã‚ÇÅ .= Œ≤‚ÇÅ .* ùîº‚àám‚Çú‚Çã‚ÇÅ .+ (1-Œ≤‚ÇÅ) .* ùîº‚àáœï‚Çú‚Çã‚ÇÅ
    glp_mat = mapreduce( grad_logp, hcat, eachcol(q) )
    norm_œï = compute_dKL(Val(:RKHS_norm), kernel, q, grad_logp=grad_logp, œï=œï)

    aux_vars[:ùîº‚àám‚Çú‚Çã‚ÇÅ] .= ùîº‚àám‚Çú‚Çã‚ÇÅ

    dKL = Œ≤‚ÇÅ.*(ùîº‚àám‚Çú‚Çã‚ÇÅ.+ sum(m‚Çú‚Çã‚ÇÅ'*glp_mat)./N) .- (1-Œ≤‚ÇÅ).*norm_œï
    return -dKL[1]
end

function update!(::Val{:scalar_Adam}, q, œï, œµ, kernel, grad_logp, aux_vars;
                 kwargs...)
    iter = get(kwargs, :iter, false)
    Œ≤‚ÇÅ = get(kwargs, :Œ≤‚ÇÅ, false)
    Œ≤‚ÇÇ = get(kwargs, :Œ≤‚ÇÇ, false)

    d, N = size(q)
    h = 1/kernel.transform.s[1]^2
    glp_mat = mapreduce( grad_logp, hcat, eachcol(q) )
    ‚àák = -1 .* kernel_grad_matrix(kernel, q)  # -1 because we need ‚àá wrt 2‚Åø·µà arg
    k_mat = KernelFunctions.kernelmatrix(kernel, q)

    aux_vars[:ùîº‚àáœï‚Çú‚Çã‚ÇÅ] .= N^2 \ (
        sum( k_mat .* (d/h .- 1/h^2 .* pairwise(SqEuclidean(), q)) )
        + sum(‚àák'*glp_mat)
                       )

    œï .= calculate_phi_vectorized(kernel, q, grad_logp; kwargs...)
    aux_vars[:m‚Çú‚Çã‚ÇÅ] .= aux_vars[:m‚Çú]
    aux_vars[:m‚Çú] .= Œ≤‚ÇÅ .* aux_vars[:m‚Çú] + (1-Œ≤‚ÇÅ) .* œï
    aux_vars[:v‚Çú] .= Œ≤‚ÇÇ .* aux_vars[:v‚Çú] + (1-Œ≤‚ÇÇ) .* œï.^2
    œµ .= œµ.*sqrt((1-Œ≤‚ÇÇ^iter)./(1-Œ≤‚ÇÅ^iter)) ./ mean(sqrt.(aux_vars[:v‚Çú]))
    q .+= œµ .* aux_vars[:m‚Çú]./(1-Œ≤‚ÇÅ^iter)
end

function update!(::Val{:scalar_RMS_prop}, q, œï, œµ, kernel, grad_logp, aux_vars;
                 kwargs...)
    Œ≥ = get(kwargs, :Œ≥, false)
    œï .= calculate_phi_vectorized(kernel, q, grad_logp; kwargs...)
    aux_vars[:G‚Çú] .= Œ≥ * norm(œï)^2 .+ (1-Œ≥) * aux_vars[:G‚Çú]
    N = size(œï, 2)
    œµ .= N*œµ/(aux_vars[:G‚Çú][1] + 1)
    q .+= œµ .*œï
end

function update!(::Val{:scalar_adagrad}, q, œï, œµ, kernel, grad_logp, aux_vars;
                 kwargs...)
    œï .= calculate_phi_vectorized(kernel, q, grad_logp; kwargs...)
    aux_vars[:G‚Çú] .+= norm(œï)^2
    N = size(œï, 2)
    œµ .= N*œµ/(aux_vars[:G‚Çú][1] + 1)
    q .+= œµ .*œï
end

function update!(::Val{:naive_WNES}, q, œï, œµ, kernel, grad_logp, aux_vars;
                 kwargs...)
    aux_vars[:q‚Çú‚Çã‚ÇÇ] .= aux_vars[:q‚Çú‚Çã‚ÇÅ]
    aux_vars[:q‚Çú‚Çã‚ÇÅ] .= q
    œï .= WNes_œï(œµ, q, aux_vars[:q‚Çú‚Çã‚ÇÅ], kernel, kwargs[:c‚ÇÅ],
               kwargs[:c‚ÇÇ], grad_logp; kwargs...)
    q .+= œµ .* œï
end

function WNes_œï(œµ, q, q‚Çú‚Çã‚ÇÅ, kernel, c‚ÇÅ, c‚ÇÇ, grad_logp; kwargs...)
    CŒîq = c‚ÇÅ*(c‚ÇÇ-1).*(q.-q‚Çú‚Çã‚ÇÅ)
    œµ.\CŒîq .+ calculate_phi_vectorized(kernel, q.+CŒîq, grad_logp; kwargs...)
end

function divergence(F, X)
    div = 0
    for i in 1:length(X)
        x_top = X[1:i-1]
        x_bot = X[i+1:end]
        f(x) = F(vcat(x_top, x, x_bot))[i]
        div += ForwardDiff.derivative(f, X[i])
    end
    return div
end

function WNes_dKL(kernel, q, œï, grad_logp, aux_vars, œµ; kwargs...)
    c‚ÇÅ = get(kwargs, :c‚ÇÅ, false)
    c‚ÇÇ = get(kwargs, :c‚ÇÇ, false)
    C = c‚ÇÅ*(c‚ÇÇ-1)
    N = size(q, 2)
    h = 1/kernel.transform.s[1]^2
    d = size(q)[1]
    t(x, x‚Çú‚Çã‚ÇÅ) = (1+C).*x .- x‚Çú‚Çã‚ÇÅ
    y = map(t, q, aux_vars[:q‚Çú‚Çã‚ÇÅ])
    dKL = 0

    k_mat = KernelFunctions.kernelmatrix(kernel, q)
    glp_mat = mapreduce( grad_logp, hcat, eachcol(q) )

    dKL += sum(œï'*glp_mat)

    glp_mat = mapreduce( grad_logp, hcat, eachcol(y) )
    ‚àák = kernel_grad_matrix(kernel, y)
    dKL += sum(‚àák'*glp_mat ) / N

    dKL += N \ sum( k_mat .* ( 2*d/h .- 4/h^2 .* pairwise(SqEuclidean(), y) ) )

    for x‚Çú‚Çã‚ÇÅ in eachcol(aux_vars[:q‚Çú‚Çã‚ÇÅ])
        function œïÃÇ(x)
            CŒîq = c‚ÇÅ*(c‚ÇÇ-1).*(x.-x‚Çú‚Çã‚ÇÅ)
            œµ.\CŒîq .+ calculate_phi(kernel, x.+CŒîq, grad_logp; kwargs...)
        end
        dKL += divergence(œïÃÇ, x‚Çú‚Çã‚ÇÅ)
    end

    return dKL/N
end

function update!(::Val{:naive_WAG}, q, œï, œµ, kernel, grad_logp, aux_vars;
                 kwargs...)
    # aux_vars[:q‚Çú‚Çã‚ÇÅ] = copy(q)
    iter = get(kwargs, :iter, false)
    Œ± = get(kwargs, :Œ±, false)
    œï .= calculate_phi_vectorized(kernel, aux_vars[:y], grad_logp; kwargs...)
    q_new = aux_vars[:y] .+ œµ.*œï
    aux_vars[:y] .= q_new .+ (iter-1)/iter .* (aux_vars[:y].-q) + (iter + Œ± -2)/iter * œµ .* œï
    q .= q_new
end

function update!(::Val{:forward_euler}, q, œï, œµ, kernel, grad_logp, aux_vars;
                 kwargs...)
    œï .= calculate_phi_vectorized(kernel, q, grad_logp; kwargs...)
    q .+= œµ.*œï
end

function calculate_phi_vectorized(kernel, q, grad_logp; kwargs...)
    Œ≥‚Çê = get(kwargs, :Œ≥‚Çê, [1.])
    N = size(q, 2)
    k_mat = KernelFunctions.kernelmatrix(kernel, q)
    grad_k = kernel_grad_matrix(kernel, q)
    glp_mat = mapreduce( grad_logp, hcat, (eachcol(q)) )
    œï = 1/N * (Œ≥‚Çê .* glp_mat * k_mat .+ grad_k )
end

function calculate_phi(kernel, q, grad_logp; kwargs...)
    glp = grad_logp.(eachcol(q))
    œï = zero(q)
    for (i, xi) in enumerate(eachcol(q))
        for (xj, glp_j) in zip(eachcol(q), glp)
            œï[:, i] .+= kernel(xj, xi) * glp_j .+ kernel_gradient(kernel, xj, xi)
            # d = kernel(xj, xi) * glp_j
            # K = kernel_gradient( kernel, xj, xi )
            # œï[:, i] .+= d .+ K
        end
    end
    œï ./= size(q, 2)
end

function compute_dKL(::Val{:KSD}, kernel::Kernel, q; grad_logp, kwargs...)
    d, N = size(q)
    h = 1/kernel.transform.s[1]^2
    k_mat = KernelFunctions.kernelmatrix(kernel, q)
    dKL = 0
    for (i, x) in enumerate(eachcol(q))
        glp_x = grad_logp(x)
        for (j, y) in enumerate(eachcol(q))
            glp_y = grad_logp(y)
            dKL += (
                    (glp_x .- (x.-y)./h) ‚ãÖ (glp_y .+ (x.-y)./h) + d/h
                   ) * k_mat[i,j]
        end
    end
    -dKL / N^2
end

function compute_dKL(::Val{:uKSD}, kernel::Kernel, q; grad_logp, kwargs...)
    d, N = size(q)
    h = 1/kernel.transform.s[1]^2
    k_mat = KernelFunctions.kernelmatrix(kernel, q)
    dKL = 0
    for (i, x) in enumerate(eachcol(q))
        glp_x = grad_logp(x)
        for (j, y) in enumerate(eachcol(q))
            if i != j
                glp_y = grad_logp(y)
                dKL += (
                        (glp_x .- (x.-y)./h) ‚ãÖ (glp_y .+ (x.-y)./h) + d/h
                       ) * k_mat[i,j]
            end
        end
    end
    -dKL / (N*(N-1))
end

function compute_dKL(::Val{:RKHS_norm}, kernel::Kernel, q; œï, kwargs...)
    if size(q)[1] == 1
        - invquad(kernelpdmat(kernel, q), vec(œï))
    else
        # this first method tries to flatten the tensor equation
        # invquad(flat_matrix_kernel_matrix(kernel, q), vec(œï))
        # the second method should be the straight forward case for a
        # kernel that is a scalar f(x) times identity matrix
        norm = 0
        try
            k_mat = kernelpdmat(kernel, q)
            for f in eachrow(œï)
                norm += invquad(k_mat, vec(f))
            end
            return - norm
        catch e
            if e isa PosDefException
                @show kernel
            end
            rethrow(e)
        end
    end
end

# not being used, double check before using another kernel
# function kernel_grad_matrix(kernel::KernelFunctions.Kernel, q)
#     if size(q)[end] == 1
#         return 0
#     end
#     grad(f,x,y) = gradient(f,x,y)[1]
# 	grad_k = mapslices(x -> grad.(kernel, [x], eachcol(q)), q, dims = 1)
#     sum(grad_k, dims=2)
# end

# gradient of k(x,y) = exp(-‚Äñx-y‚Äñ¬≤/2h) with respect to x
function kernel_gradient(k::TransformedKernel{SqExponentialKernel}, x, y)
    - k.transform.s[1]^2 .* (x-y) .* k(x,y)
end

function kernel_grad_matrix(kernel::TransformedKernel{SqExponentialKernel}, q)
    ‚àák = zeros(size(q))
    for (j, y) in enumerate(eachcol(q))
        for (i, x) in enumerate(eachcol(q))
            ‚àák[:,j] .+= kernel_gradient(kernel, x, y)
        end
    end
    ‚àák
end

export kernel_grad_matrix

using DrWatson
using ProgressMeter
using Statistics
using ValueHistories
using KernelFunctions
using LinearAlgebra
using Random
using Flux
using ForwardDiff
using Distances
using PDMats

export svgd_fit
export calculate_phi_vectorized
export compute_dKL
export kernel_grad_matrix
export calculate_phi

function svgd_fit(q, grad_logp; kwargs...)
    kernel = TransformedKernel(SqExponentialKernel(), ScaleTransform(1.))
    kwargs = Dict(kwargs...)
    callback = get(kwargs, :callback, nothing)
    n_iter = get(kwargs, :n_iter, 1)
    n_particles = get(kwargs, :n_particles, 1)
    step_size = get(kwargs, :step_size, 1)
    kernel_cb! = get!(kwargs, :kernel_cb, nothing)
    step_size_cb = get!(kwargs, :step_size_cb, nothing)
    update_method = get!(kwargs, :update_method, :forward_euler)
    annealing_schedule = get!(kwargs, :annealing_schedule, nothing)
    annealing_params = get!(kwargs, :annealing_params, [])
    progress = get(kwargs, :progress, true)

    aux_vars = Dict()
    if update_method in [:scalar_adagrad, :scalar_RMS_prop]
        aux_vars[:Gâ‚œ] = [0.]
    elseif update_method == :scalar_Adam
        aux_vars[:mâ‚œ] = zeros(size(q))
        aux_vars[:mâ‚œâ‚‹â‚] = zeros(size(q))
        aux_vars[:vâ‚œ] = zeros(size(q))
        aux_vars[:ð”¼âˆ‡mâ‚œâ‚‹â‚] = [0.]
        aux_vars[:ð”¼âˆ‡Ï•â‚œâ‚‹â‚] = [0.]
    elseif update_method in [:WAG, :WNES]
        aux_vars[:y] = copy(q)
        aux_vars[:qâ‚œâ‚‹â‚] = copy(q)
    end
    hist = MVHistory()
    Ï• = zeros(size(q))
    progress ? p = Progress(n_iter, 1) : nothing
    for i in 1:n_iter
        isnothing(kernel_cb!) ? nothing : kernel_cb!(kernel, q)
        Ïµ = isnothing(step_size_cb) ? [step_size] : [step_size_cb(step_size, i)]
        Î³â‚ = if isnothing(annealing_schedule)
            [1.]
        else
            [annealing_schedule(i, n_iter; annealing_params...)]
        end
        push!(hist, :annealing, i, Î³â‚[1])
        âˆ‡logp_mat = mapreduce(grad_logp, hcat, eachcol(q))
        store_dKL!(hist, q, Ïµ, Ï•, i, Î³â‚, kernel, grad_logp, âˆ‡logp_mat,
                      ;kwargs..., aux_vars...)
        update!(Val(update_method), q, Ï•, Ïµ, kernel, âˆ‡logp_mat, t=i, Î³â‚=Î³â‚
                ;aux_vars..., kwargs...)
        push!(hist, :Ï•_norm, i, mean(norm(Ï•)))
        push!(hist, :step_sizes, i, Ïµ[1])
        if !isnothing(callback)
            callback(;hist=hist, q=q, Ï•=Ï•, i=i, kernel=kernel, âˆ‡logp_mat,
                     aux_vars..., kwargs...)
        end
        progress ? next!(p) : nothing
    end
    return q, hist
end

function store_dKL!(
    hist, q, Ïµ, Ï•, i, Î³â‚, kernel, grad_logp, âˆ‡logp_mat,
    ;dKL_estimator=nothing, kwargs...
)
    if kwargs[:update_method] âˆˆ [:WAG, :WNES] || isnothing(dKL_estimator)
        nothing
    elseif kwargs[:update_method] == :scalar_Adam
        dKL = dKL_Adam(kernel, q, Ï•, grad_logp, Ïµ, âˆ‡logp_mat; kwargs...)
        push!(hist, :adam_dKL, i, dKL)
    elseif typeof(dKL_estimator) == Symbol
        dKL = compute_dKL(Val(dKL_estimator), kernel, q, âˆ‡logp_mat, Ï•=Ï•,
                          grad_logp=grad_logp)
        dKL += dKL_annealing_correction(Ï•, âˆ‡logp_mat, q, Î³â‚)
        push!(hist, dKL_estimator, i, dKL)
    elseif typeof(dKL_estimator) == Array{Symbol,1}
        for estimator in dKL_estimator
            dKL = compute_dKL(Val(estimator), kernel, q, Ï•=Ï•, grad_logp=grad_logp)
            dKL += dKL_annealing_correction(Ï•, grad_logp, q, Î³â‚)
            push!(hist, estimator, i, dKL)
        end
    end
    push!(hist, :kernel_width, kernel.transform.s)
end

function dKL_annealing_correction(Ï•, âˆ‡logp_mat, q, Î³â‚)
    -(1-Î³â‚[1])*Ï•â‹…âˆ‡logp_mat/size(q, 2)
end

function dKL_Adam(kernel, q, Ï•, grad_logp, Ïµ, âˆ‡logp_mat
;ð”¼âˆ‡mâ‚œâ‚‹â‚, ð”¼âˆ‡Ï•â‚œâ‚‹â‚, Î²â‚, Î²â‚‚, mâ‚œâ‚‹â‚, kwargs...
)
    N = size(q, 2)
    ð”¼âˆ‡mâ‚œâ‚‹â‚ .= Î²â‚ .* ð”¼âˆ‡mâ‚œâ‚‹â‚ .+ (1-Î²â‚) .* ð”¼âˆ‡Ï•â‚œâ‚‹â‚
    norm_Ï• = RKHS_norm(kernel, q, Ï•=Ï•)
    dKL = Î²â‚.*(ð”¼âˆ‡mâ‚œâ‚‹â‚.+mâ‚œâ‚‹â‚â‹…âˆ‡logp_mat/N).+(1-Î²â‚).*norm_Ï•
    return -dKL[1]
end

function update!(::Val{:scalar_Adam},
q, Ï•, Ïµ, kernel, âˆ‡logp_mat
;t, mâ‚œâ‚‹â‚, mâ‚œ, vâ‚œ, ð”¼âˆ‡Ï•â‚œâ‚‹â‚, Î²â‚, Î²â‚‚, adam_stepsize_method=:average, kwargs...
)
    # We must compute ð”¼âˆ‡Ï• here because we need it for dKL/dt later.
    ð”¼âˆ‡Ï•â‚œâ‚‹â‚ .= ð”¼âˆ‡Ï•(kernel, q, âˆ‡logp_mat; kwargs...)

    Ï• .= calculate_phi_vectorized(kernel, q, âˆ‡logp_mat)
    mâ‚œâ‚‹â‚ .= mâ‚œ
    mâ‚œ .= Î²â‚ .* mâ‚œ + (1-Î²â‚) .* Ï•
    vâ‚œ .= Î²â‚‚ .* vâ‚œ + (1-Î²â‚‚) .* Ï•.^2

    if adam_stepsize_method == :average
        Ïµ .*= sqrt(1-Î²â‚‚^t)./(1-Î²â‚^t) .* mean(1.0./sqrt.(vâ‚œ).+1)
    elseif adam_stepsize_method == :minimum
        Ïµ .*= sqrt(1-Î²â‚‚^t)./(1-Î²â‚^t) .* 1.0/sqrt.(maximum(vâ‚œ).+1)
    end

    q .+= Ïµ .* mâ‚œ
end

function ð”¼âˆ‡Ï•(kernel, q, âˆ‡logp_mat; adam_unbiased=false, kwargs...)
    d, N = size(q)
    h = 1/kernel.transform.s[1]^2
    k_mat = KernelFunctions.kernelmatrix(kernel, q)
    âˆ‡k = -1.0.*kernel_grad_matrix(kernel, q)
    # Multiply by -1 because we need the gradient, âˆ‡, with respect to the
    # second argument and for the RBF kernel that is -1 times the gradient
    # with respect to the first argument.
    if adam_unbiased
        ð”¼âˆ‡Ï• = (
               sum(k_mat .* (d/h .- 1/h^2 .* pairwise(SqEuclidean(), q)))
               - sum(diag(k_mat .* (d/h .- 1/h^2 .* pairwise(SqEuclidean(), q))))
               +sum(âˆ‡k'*âˆ‡logp_mat) - âˆ‡kâ‹…âˆ‡logp_mat
              ) / (N*(N-1))
    else
        ð”¼âˆ‡Ï• = N^2\(sum(k_mat .* (d/h .- 1/h^2 .* pairwise(SqEuclidean(), q)))
                   +sum(âˆ‡k'*âˆ‡logp_mat)
                  )
    end
    return ð”¼âˆ‡Ï•
end

function update!(::Val{:scalar_RMS_prop},
q, Ï•, Ïµ, kernel, âˆ‡logp_mat
;Gâ‚œ, Î³, kwargs...
)
    Ï• .= calculate_phi_vectorized(kernel, q, âˆ‡logp_mat; kwargs...)
    Gâ‚œ .= Î³ * norm(Ï•)^2 .+ (1-Î³) * Gâ‚œ
    Ïµ .= Ïµ/(âˆš(Gâ‚œ[1] + 1))
    q .+= Ïµ.*Ï•
end

function update!(::Val{:scalar_adagrad},
q, Ï•, Ïµ, kernel, âˆ‡logp_mat
; Gâ‚œ, kwargs...
)
    Ï• .= calculate_phi_vectorized(kernel, q, âˆ‡logp_mat; kwargs...)
    Gâ‚œ .+= norm(Ï•)^2
    N = size(Ï•, 2)
    Ïµ .= N*Ïµ/(Gâ‚œ[1] + 1)
    q .+= Ïµ .*Ï•
end

function update!(::Val{:WAG},
q, Ï•, Ïµ, kernel, âˆ‡logp_mat
;t, qâ‚œâ‚‹â‚, y, Î±, kwargs...
)
    Ï• .= calculate_phi_vectorized(kernel, y, âˆ‡logp_mat)
    q .= y.+Ïµ.*Ï•
    y .= q + (t-1)/t.*(y.-qâ‚œâ‚‹â‚) + (t+Î±-2)/t*Ïµ.*Ï•
    qâ‚œâ‚‹â‚ .= q
end

function update!(::Val{:forward_euler},
q, Ï•, Ïµ, kernel, âˆ‡logp_mat,
;kwargs...
)
    Ï• .= calculate_phi_vectorized(kernel, q, âˆ‡logp_mat)
    q .+= Ïµ.*Ï•
end

function update!(::Val{:WNES},
q, Ï•, Ïµ, kernel, âˆ‡logp_mat
;câ‚, qâ‚œâ‚‹â‚, y, câ‚‚, kwargs...
)
    Ï• .= calculate_phi_vectorized(kernel, y, âˆ‡logp_mat)
    q .= y.+Ïµ.*Ï•
    y .= q .+ câ‚*(câ‚‚-1).*(q.-qâ‚œâ‚‹â‚)
    qâ‚œâ‚‹â‚ .= q
end

function divergence(F, X)
    div = 0
    for i in 1:length(X)
        x_top = X[1:i-1]
        x_bot = X[i+1:end]
        f(x) = F(vcat(x_top, x, x_bot))[i]
        div += ForwardDiff.derivative(f, X[i])
    end
    return div
end

function WNes_dKL(kernel, q, Ï•, grad_logp, aux_vars, Ïµ, âˆ‡logp_mat; kwargs...)
    # câ‚ = get(kwargs, :câ‚, false)
    # câ‚‚ = get(kwargs, :câ‚‚, false)
    # C = câ‚*(câ‚‚-1)
    # N = size(q, 2)
    # h = 1/kernel.transform.s[1]^2
    # d = size(q)[1]
    # t(x, xâ‚œâ‚‹â‚) = (1+C).*x .- xâ‚œâ‚‹â‚
    # y = map(t, q, aux_vars[:qâ‚œâ‚‹â‚])
    # dKL = 0

    # k_mat = KernelFunctions.kernelmatrix(kernel, q)

    # # This sums over all combinations of particles, not sure whether that is
    # # correct, but since WNes_dKL is probably not correct at all anyway it
    # # doesn't matter for now.
    # dKL += sum(Ï•'*âˆ‡logp_mat)

    # âˆ‡k = kernel_grad_matrix(kernel, y)
    # # See comment above.
    # dKL += sum(âˆ‡k'*âˆ‡logp_mat ) / N

    # dKL += N \ sum( k_mat .* ( 2*d/h .- 4/h^2 .* pairwise(SqEuclidean(), y) ) )

    # for xâ‚œâ‚‹â‚ in eachcol(aux_vars[:qâ‚œâ‚‹â‚])
    #     function Ï•Ì‚(x)
    #         CÎ”q = câ‚*(câ‚‚-1).*(x.-xâ‚œâ‚‹â‚)
    #         Ïµ.\CÎ”q .+ calculate_phi(kernel, x.+CÎ”q, grad_logp, âˆ‡logp_mat;
    #                                 kwargs...)
    #     end
    #     dKL += divergence(Ï•Ì‚, xâ‚œâ‚‹â‚)
    # end

    # return dKL/N
    0.
end

function calculate_phi_vectorized(kernel, q, âˆ‡logp_mat;Î³â‚=[1.], kwargs...)
    N = size(q, 2)
    k_mat = KernelFunctions.kernelmatrix(kernel, q)
    grad_k = kernel_grad_matrix(kernel, q)
    Ï• = 1/N * (Î³â‚ .* âˆ‡logp_mat * k_mat .+ grad_k)
end

function calculate_phi(kernel, q, grad_logp, âˆ‡logp_mat; kwargs...)
    # glp = grad_logp.(eachcol(q))
    @warn "Using âˆ‡logp_mat here is untested."
    glp = âˆ‡logp_mat
    Ï• = zero(q)
    for (i, xi) in enumerate(eachcol(q))
        for (xj, glp_j) in zip(eachcol(q), glp)
            Ï•[:, i] .+= kernel(xj, xi) * glp_j .+ kernel_gradient(kernel, xj, xi)
            # d = kernel(xj, xi) * glp_j
            # K = kernel_gradient( kernel, xj, xi )
            # Ï•[:, i] .+= d .+ K
        end
    end
    Ï• ./= size(q, 2)
end

function compute_dKL(::Val{:KSD}, kernel::Kernel, q, âˆ‡logp_mat
;kwargs...
)
    d, N = size(q)
    h = 1/kernel.transform.s[1]^2
    k_mat = KernelFunctions.kernelmatrix(kernel, q)
    dKL = 0
    for (i, x) in enumerate(eachcol(q))
        glp_x = âˆ‡logp_mat[:, i]
        for (j, y) in enumerate(eachcol(q))
            glp_y = âˆ‡logp_mat[:, j]
            dKL += (
                    (glp_x .- (x.-y)./h) â‹… (glp_y .+ (x.-y)./h) + d/h
                   ) * k_mat[i,j]
        end
    end
    -dKL / N^2
end

function compute_dKL(::Val{:uKSD}, kernel::Kernel, q, âˆ‡logp_mat
;kwargs...
)
    d, N = size(q)
    h = 1/kernel.transform.s[1]^2
    k_mat = KernelFunctions.kernelmatrix(kernel, q)
    dKL = 0
    for (i, x) in enumerate(eachcol(q))
        glp_x = âˆ‡logp_mat[:, i]
        for (j, y) in enumerate(eachcol(q))
            if i != j
                glp_y = âˆ‡logp_mat[:, j]
                dKL += (
                        (glp_x .- (x.-y)./h) â‹… (glp_y .+ (x.-y)./h) + d/h
                       ) * k_mat[i,j]
            end
        end
    end
    -dKL / (N*(N-1))
end

function compute_dKL(::Val{:RKHS_norm}, kernel::Kernel, q, args...; Ï•, kwargs...)
    return -RKHS_norm(kernel, q; Ï•=Ï•, kwargs...)
end

function RKHS_norm(kernel::Kernel, q; Ï•, kwargs...)
    if size(q)[1] == 1
        invquad(kernelpdmat(kernel, q), vec(Ï•))
    else
        # this first method tries to flatten the tensor equation
        # invquad(flat_matrix_kernel_matrix(kernel, q), vec(Ï•))
        # the second method should be the straight forward case for a
        # kernel that is a scalar f(x) times identity matrix
        norm = 0
        try
            k_mat = kernelpdmat(kernel, q)
            for f in eachrow(Ï•)
                norm += invquad(k_mat, vec(f))
            end
            return norm
        catch e
            if e isa PosDefException
                @show kernel
            end
            rethrow(e)
        end
    end
end

# not being used, double check before using another kernel
# function kernel_grad_matrix(kernel::KernelFunctions.Kernel, q)
#     if size(q)[end] == 1
#         return 0
#     end
#     grad(f,x,y) = gradient(f,x,y)[1]
# 	grad_k = mapslices(x -> grad.(kernel, [x], eachcol(q)), q, dims = 1)
#     sum(grad_k, dims=2)
# end

# gradient of k(x,y) = exp(-â€–x-yâ€–Â²/2h) with respect to x
function kernel_gradient(k::TransformedKernel{SqExponentialKernel}, x, y)
    - k.transform.s[1]^2 .* (x-y) .* k(x,y)
end

function kernel_grad_matrix(kernel::TransformedKernel{SqExponentialKernel}, q)
    âˆ‡k = zeros(size(q))
    for (j, y) in enumerate(eachcol(q))
        for (i, x) in enumerate(eachcol(q))
            âˆ‡k[:,j] .+= kernel_gradient(kernel, x, y)
        end
    end
    âˆ‡k
end
